# moe-experiments
This project implements a simple Mixture of Experts (MoE) Transformer in PyTorch for educational purposes. Inspired by LLaMA-4 architecture, it features top-k expert routing, modular design, and a mini dataset pipeline using Tiny Shakespeare dataset.

This repository is inspired by FareedKhan-dev/train-llama4, which provides a didactic implementation of a Llama-4-based MoE model.

Test git push
