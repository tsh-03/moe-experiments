# ğŸ§  MoE Experiments â€“ Mixture of Experts Transformer in PyTorch  
*A minimal and educational implementation of a Mixture of Experts (MoE) Transformer inspired by LLaMAâ€‘4.*  

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)]()
[![PyTorch](https://img.shields.io/badge/PyTorch-%23EE4C2C.svg?&logo=PyTorch&logoColor=white)]()

---

## ğŸ“š Table of Contents
- [ğŸ“– About the Project](#-about-the-project)
- [ğŸ“‚ Repository Structure](#-repository-structure)
- [ğŸ§  Mixture of Experts (MoE)](#-mixture-of-experts-moe)
- [ğŸ“Š Metrics to Analyze MoE](#-metrics-to-analyze-moe-behavior)
- [ğŸ“œ Dataset: TinyStories](#-tinystories-dataset)
- [ğŸš€ Quick Start](#-quick-start)
- [ğŸ“ˆ Experiments & Results](#-experiments--results)
- [ğŸ”® Future Work](#-future-work)
- [ğŸ“– Citation](#-citation)

---

## ğŸ“– About the Project
This repository implements a **Mixture of Experts (MoE) Transformer** in PyTorch for educational purposes.  
It is inspired by **LLaMAâ€‘4's MoE architecture**, featuring:

âœ… **Topâ€‘k expert routing (Topâ€‘1, Topâ€‘2, Random)**  
âœ… Modular design for easy experimentation  
âœ… Mini dataset pipeline using the **TinyStories dataset**  
âœ… Detailed analysis of **routing entropy, expert utilization, and specialization**  

The goal is to provide **handsâ€‘on understanding** of how MoE models work, making it a **starting point for scaling to larger LLMs**.

---

## ğŸ“‚ Repository Structure

| File | Description |
|------|------------|
| `model.py` | Defines the MoE Transformer architecture, router, experts, and forward logic |
| `prepare_data.py` | Data preparation utilities (character-level & tiktoken tokenizers) |
| `train.py` | Training loop with loss computation, accuracy metrics, and logging |
| `utils.py` | Helper functions for saving/loading models and utilities |
| `moe-transformer.ipynb` | Notebook for training and visualizing loss/metrics |
| `moe-analyze.ipynb` | Notebook for analyzing expert utilization and entropy |
| `saved_models/` | Folder to store trained checkpoints |
| `LICENSE` | MIT License |
| `README.md` | This file |

---

## ğŸ§  Mixture of Experts (MoE)

**MoE (Mixture of Experts)** is a technique where a **router dynamically selects a subset of experts (MLPs)** to process each token.  
This allows **increased model capacity** without increasing perâ€‘token computation.

ğŸ”¹ **Key Idea:**  
- Router assigns each token to **Topâ€‘k experts**  
- Only selected experts compute forward pass  
- Improves **efficiency, scalability, and specialization**

---

### ğŸ”€ Routing Strategies
| Routing Type   | Description |
|---------------|------------|
| **Topâ€‘1**     | Each token routed to the most probable expert |
| **Topâ€‘2**     | Each token routed to 2 experts â†’ smoother gradients |
| **Random**    | Experts selected randomly (sanity check) |

---

## ğŸ“Š Metrics to Analyze MoE Behavior

### 1ï¸âƒ£ **Routing Entropy**
Measures router **confidence** in expert selection.

- Low entropy â†’ router is confident (may be overconfident)  
- High entropy â†’ router is uncertain

Formula:  
$$H_i = -\sum_{j=1}^{E} p_{i,j} \log(p_{i,j}+\epsilon)$$

---

### 2ï¸âƒ£ **Expert Utilization**
Measures how **evenly tokens are distributed** among experts.

- Low std dev â†’ Balanced expert usage  
- High std dev â†’ Some experts dominate  

Formula:  
$$Utilization_i = \frac{\text{tokens sent to expert } i}{\text{total tokens}}$$

---

## ğŸ“œ TinyStories Dataset
We use [TinyStories](https://huggingface.co/datasets/roneneldan/TinyStories) â€“ a collection of **short, simple stories** perfect for rapid prototyping and analyzing expert routing behaviors in MoE models.

---

## ğŸš€ Quick Start

### ğŸ”§ Installation
```bash
git clone https://github.com/<your-username>/moe-experiments.git
cd moe-experiments
pip install .
