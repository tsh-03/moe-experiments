# moe-experiments
This project implements a simplified Mixeture of Experts (MoE) Transformer architecture, adapted into a minimal PyTorch prototype for experimentation.
