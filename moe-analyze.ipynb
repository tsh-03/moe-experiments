{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4715ec9a",
   "metadata": {},
   "source": [
    "### Mixture of Experts (MoE) Transformer with Llama4 type model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7bf2eb",
   "metadata": {},
   "source": [
    "Author: Tirth Shah  \n",
    "Inspired by: https://github.com/FareedKhan-dev/train-llama4\n",
    "\n",
    "In this notebook, we perform inference with the trained model saved in saved_models directory. We also analyze the MoE layer in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548f6785",
   "metadata": {},
   "source": [
    "#### Import Required Libraries and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40e88d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tirthshah/miniforge3/envs/moe-experiments/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Import our custom modules\n",
    "from model import MoETransformer, ModelConfig\n",
    "from prepare_data import CharDataset, TinyStoriesDataset, sample_alice_text\n",
    "from train import TrainModel\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f467e3d4",
   "metadata": {},
   "source": [
    "Select device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7b2c1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81802784",
   "metadata": {},
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36810c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from ./saved_models/try.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MoETransformer(\n",
       "  (token_embedding_table): Embedding(36, 128)\n",
       "  (rope): RoPE()\n",
       "  (rms_norm): ModuleList(\n",
       "    (0-4): 5 x RMSNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (attention_layers): ModuleList(\n",
       "    (0-3): 4 x MultiHeadAttention(\n",
       "      (rope): RoPE()\n",
       "      (qkv_proj): Linear(in_features=128, out_features=384, bias=False)\n",
       "      (out_proj): Linear(in_features=128, out_features=128, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (moe_layers): ModuleList(\n",
       "    (0-3): 4 x MoELayer(\n",
       "      (router_linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "      (shared_gate): Linear(in_features=128, out_features=256, bias=False)\n",
       "      (shared_up): Linear(in_features=128, out_features=256, bias=False)\n",
       "      (shared_down): Linear(in_features=256, out_features=128, bias=False)\n",
       "      (activation_fn): SiLU()\n",
       "    )\n",
       "  )\n",
       "  (final_output_layer): Linear(in_features=128, out_features=36, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the trained MoETransformer model\n",
    "model_path = './saved_models/try.pth'\n",
    "\n",
    "model, train_config, train_losses, routing_entropies = utils.load_model(model_path)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad90fde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataset on which the model was trained\n",
    "if model.config.dataset_tag == 'sample_alice':\n",
    "    block_size = 64 # Define the block size for the dataset\n",
    "    dataset = CharDataset(text=sample_alice_text, block_size=block_size) # Create dataset\n",
    "\n",
    "elif model.config.dataset_tag == 'tiny_stories':\n",
    "    block_size = 64 # Define the block size for the dataset\n",
    "    dataset = TinyStoriesDataset(block_size=block_size, max_samples=100000)  # Load the dataset\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Invalid dataset_tag. Choose 'sample_alice' or 'tiny_stories'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46778c17",
   "metadata": {},
   "source": [
    "Generate sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd71aa75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation loop finished.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Aliceiusnk.(w: sg'rhlvv(,,pok-guce?t)v-hfAWonok-ipsmhfhkat:mkA y\\na'i(\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(\n",
    "    seed_text=\"Alice\",\n",
    "    tokenizer=dataset.tokenizer,\n",
    "    max_new_tokens=model.config.block_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b875594",
   "metadata": {},
   "source": [
    "Analyze MoE Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16241837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.2894741296768188,\n",
       "  1.2726950645446777,\n",
       "  1.279577612876892,\n",
       "  1.2405662536621094],\n",
       " [1.290470838546753,\n",
       "  1.2723817825317383,\n",
       "  1.2805781364440918,\n",
       "  1.2435355186462402]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "routing_entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4400443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Routing Entropy: 1.2794060707092285\n",
      "Expert Utilization: tensor([34, 36, 21, 37])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Routing Entropy: {model.moe_layers[0].compute_routing_entropy()}\")\n",
    "\n",
    "print(f\"Expert Utilization: {model.moe_layers[0].compute_expert_utilization()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moe-experiments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
