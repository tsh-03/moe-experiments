[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"

[project]
name = "MoE-Experiments"
version = "0.1"
authors = [
  { name="Tirth Shah", email="tirth.shah@qant.gmbh" },
]
description = "This project implements a simple Mixture of Experts (MoE) Transformer in PyTorch for 
educational purposes. Inspired by LLaMA-4 architecture, it features top-k expert routing, modular design, 
and a mini dataset pipeline using Tiny Stories dataset."
readme = "README.md"
requires-python = ">=3.11"
dependencies = [
    "matplotlib",
    "numpy",
    "torch",
    "tqdm",
    "datasets",
    "tiktoken",
]
classifiers = [
    "Programming Language :: Python :: 3",
    "Operating System :: OS Independent",
]